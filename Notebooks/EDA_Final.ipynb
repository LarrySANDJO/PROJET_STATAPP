{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5759b923",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e952940",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import missingno as msno # for null Values\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, LabelEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn import set_config \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f37990",
   "metadata": {},
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6aef3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"/home/onyxia/work/PROJET_STATAPP/Data/Cleans/fraud_oracle.csv\", sep=\";\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3d9320",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde1a5a9",
   "metadata": {},
   "source": [
    "We have 15420 entries and 33 variables : 9 quantitatives and 24 categorical. \n",
    "\n",
    "Data explanation : \n",
    "\n",
    "- Month: The month in which the accident actually occurred. \n",
    "\n",
    "- WeekOfMonth: The week of the month in which the accident actually occurred.\n",
    "\n",
    "- DayOfWeek: The day of the week on which the accident actually occurred.\n",
    "\n",
    "- Make: The manufacturer of the vehicle involved in the claim.\n",
    "\n",
    "- AccidentArea: The area where the accident occurred (urban/rural).\n",
    "\n",
    "- DayOfWeekClaimed: The day of the week on which the insurance claim was processed.\n",
    "\n",
    "- MonthClaimed: The month in which the insurance claim was processed.\n",
    "\n",
    "- WeekOfMonthClaimed: The week of the month in which the insurance claim was processed.\n",
    "\n",
    "- Sex: The gender of the policyholder.\n",
    "\n",
    "- MaritalStatus: The material status of the policyholder.\n",
    "\n",
    "- Age: The age of the policyholder.\n",
    "\n",
    "- Fault: Indicates whether the policyholder was at fault in the accident.\n",
    "\n",
    "- PolicyType: The type of insurance policy.\n",
    "\n",
    "- VehicleCategory: The category of the vehicle (e.g., sedan, SUV).\n",
    "\n",
    "- VehiclePrice: The price of vehicle.\n",
    "\n",
    "- FraudFound_P: Indicates whether fraud was detected in the insurance claim (our target variable)\n",
    "\n",
    "- PolicyNumber: The unique identifier for the insurance policy.\n",
    "\n",
    "- RepNumber: The unique identifier for the insurance representative handling the claim.\n",
    "\n",
    "- Deductible: The amount that the policy holder must pay out of pocket before the insurance company pays the remaining costs.\n",
    "\n",
    "- DriverRating: The rating of the driver, often based on driving history or other factors.\n",
    "\n",
    "- Days_Policy_Accident: The number of days since the policy was issued until the accident occurred.\n",
    "\n",
    "- Days_Policy_Claim: The number of days since the policy was issued until the claim was made.\n",
    "\n",
    "- PastNumberOfClaims: The number of claims previously made by the policyholder. \n",
    "\n",
    "- AgeOfVehicle: The age of the vehicle involved in the claim.\n",
    "\n",
    "- AgeOfPolicyHolder: The age of the policyholder.\n",
    "\n",
    "- PoliceReportFiled: Indicates whether a police report was filed for the accident.\n",
    "\n",
    "- WitnessPresent: Indicates whether a witness was present at the scene of the accident.\n",
    "\n",
    "- AgentType: The type of insurance agent handling the policy (e.g., internal, external)\n",
    "\n",
    "- NumberOfSuppliments: The number of supplementary documents or claims related to the main claim, categorized into ranges.\n",
    "\n",
    "- AddressChange_Claim: Indicates whether the address of the policyholder was changed at the time of the claim, categorized into ranges.\n",
    "\n",
    "- NumberOfCars: The number of cars insured under the policy, categorized into ranges.\n",
    "\n",
    "- Year: The year in which the claim was made or processed.\n",
    "\n",
    "- BasePolicy: The base policy type (e.g., Liability, Collision, All Perils)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5cba58",
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81262c0c",
   "metadata": {},
   "source": [
    "We don't have missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8392cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for duplicate values\n",
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06c9ec5",
   "metadata": {},
   "source": [
    "No duplicated value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d76eed",
   "metadata": {},
   "source": [
    "## Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cfca0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_counts = data['FraudFound_P'].value_counts()\n",
    "\n",
    "# Pie chart\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.pie(target_counts, labels=target_counts.index, autopct='%1.1f%%', startangle=90, colors=['skyblue','salmon'])\n",
    "plt.title('Distribution of FraudFound_P')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d5f50e",
   "metadata": {},
   "source": [
    "Our target Variable \"Fraud_Found_P\" is highly imbalanced. \n",
    "Oversampling address these issues by balancing the dataset, which helps the model to learn from both classes more effectively. This improves the model's ability to correctly predict the minority class, leading to better overall performance and more reliable evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8aa773",
   "metadata": {},
   "source": [
    "## Qualitative variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac332801",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = data.select_dtypes(include=['object', 'category'])\n",
    "cat_cols = cat.columns \n",
    "\n",
    "# Loop through and display detailed info for each categorical variable\n",
    "for col in cat_cols:\n",
    "    print(f\"\\n==================== {col} ====================\")\n",
    "    print(f\"Number of unique modalities: {data[col].nunique()}\")\n",
    "    print(\"\\nPercentages :\")\n",
    "    print((data[col].value_counts(normalize=True, dropna=False) * 100).round(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8275a9",
   "metadata": {},
   "source": [
    "- The features \"Sex\", \"PoliceReportFiled\", and \"WitnessPresent\" are actually Boolean Types. Should be converted to 0 or 1.\n",
    "- The features \"AccidentalArea\", \"Fault\", and \"AgentType\" each have only two unique values. Can also be converted to 0 or 1.\n",
    "- PolicyType - appears to be a concatenation of VehicleCategory and BasePolicy (so we'll drop those two)\n",
    "- Anomalies : day_of_week_claimed (8 modalities), month_claimed (13 modalities) none on days_policy_Accident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e46842",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['DayOfWeekClaimed'] == '0']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0231403b",
   "metadata": {},
   "source": [
    "We'll drop this line cause it has many anomalies (Age = 0, Monthclaimed = 0 but a WeekOfMonthClaimed that is not null, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1557887",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['DayOfWeekClaimed'] != '0']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed85293",
   "metadata": {},
   "source": [
    "Now we look at lines where Days_Policy_Accident = None. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939d3fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "none_days=data[data['Days_Policy_Accident'] == 'none']\n",
    "none_days.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a28cf35",
   "metadata": {},
   "source": [
    "None can means that the accident occured the same day the policy was issued. So we'll leave it like that. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e86ffda",
   "metadata": {},
   "source": [
    "### Relation with the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d41ffb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in cat_cols:\n",
    "    # Proportions per category\n",
    "    prop_df = pd.crosstab(data[cat], data['FraudFound_P'], normalize='index')\n",
    "    prop_df = prop_df.reset_index()\n",
    "    prop_df = prop_df.melt(id_vars=cat, value_vars=[0,1], var_name='FraudFound_P', value_name='Proportion')\n",
    "\n",
    "    # Graph\n",
    "    fig = px.bar(prop_df, x=cat,\n",
    "        y='Proportion',\n",
    "        color='FraudFound_P',\n",
    "        text='Proportion',\n",
    "        barmode='stack',\n",
    "        color_discrete_map={0:'skyblue', 1:'salmon'},\n",
    "        labels={'FraudFound_P':'Fraud', 'Proportion':'Proportion'},\n",
    "        title=f'Proportion of Fraud vs Non-Fraud by {cat}'\n",
    "    )\n",
    "\n",
    "    fig.update_traces(texttemplate='%{text:.1%}', textposition='inside')\n",
    "    fig.update_layout(yaxis_tickformat='.0%')\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff9609a",
   "metadata": {},
   "source": [
    "- Variables of dates(Month, WeekOfMonth, DayOfWeek, DayOfWeekClaimed, WeekOfMonthClaimed, MonthClaimed) seems not to influence the fraud too much\n",
    "- The Mercedes owners are more likely to be involved in fraud,with almost double the incidence compared to the second-highest group, Accura owners. On the other hand, the Porche, Lexus, Jaguar, Ferrari owners have never been reported for fraud; all the four make are \"High-End\".\n",
    "- The results suggest that males are significantly more likely to be involved in detected fraud cases compared to females. \n",
    "- Fraud is generally declared when the fault comes from the policy holder\n",
    "- Fraudulent cases were detected most frequently under the \"All Perils\". Within specified policy types, \"Sport-Collision\" had the highest fraud detection rate.\n",
    "- Fraud is most frequently detected among teenagers and retired seniors. Teenagers have weak financial power as they have not yet started their economic activities, and retired seniors experience weakened financial power post-retirement.\n",
    "- Fraudster use to change their address "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d538dbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test of chi2 \n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "for var in cat_cols:\n",
    "    # Crée la table de contingence\n",
    "    contingency_table = pd.crosstab(data[var], data['FraudFound_P'])\n",
    "    \n",
    "    # Applique le test du chi-deux\n",
    "    chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "    \n",
    "    print(f\"Variable: {var}\")\n",
    "    print(f\"Chi2 Statistic: {chi2:.2f}, p-value: {p:.4f}\")\n",
    "    print(\"-\"*40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b42ac2",
   "metadata": {},
   "source": [
    "The khi-2 test shows us that NumberofCars, WitnessPresent, Days_Policy_claim, MaritalStatus, Dayofweekclaimed, dayofweek are not significative at 5%. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248f692c",
   "metadata": {},
   "source": [
    "## Quantitative variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9ab41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "quanti = data.select_dtypes(include=['int64', 'float64'])\n",
    "quanti_cols = data.select_dtypes(include=['int64', 'float64']).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dee45ea",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06197032",
   "metadata": {},
   "outputs": [],
   "source": [
    "quanti.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e6db50",
   "metadata": {},
   "source": [
    "The minimum value of \"Age\" is 0. It totally doesn't make sense.  \n",
    "\"PolicyNumber\" and \"RepNumber\" are merely identification numbers. Let's drop them.\n",
    "\n",
    "Strictly speaking, among all the features, only Age is a numeric variable. The rest can be interpreted as categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ef71f4",
   "metadata": {},
   "source": [
    "### Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df5af0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plot with 2 subplots\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Histogram of Age\", \"Boxplot of Age\"))\n",
    "\n",
    "# Histogram\n",
    "hist = px.histogram(data, x=\"Age\", nbins=50)\n",
    "fig.add_trace(hist.data[0], row=1, col=1)\n",
    "\n",
    "# Boxplot\n",
    "box = px.box(data, y=\"Age\")\n",
    "fig.add_trace(box.data[0], row=1, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    showlegend=False,\n",
    "    height=400,\n",
    "    width=900\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27492255",
   "metadata": {},
   "source": [
    "We have some outliers but they are plausible and their percentage is not very large. \n",
    "Actually, Age and AgeofPolicyHolder seems to be the same. Let's check it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fd1afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Bornes et labels\n",
    "bins = [0, 17, 20, 25, 30, 35, 40, 50, 65, float('inf')]\n",
    "labels = ['0-17','18-20','21-25', '26-30', '31-35', '36-40', '41-50', '51-65', '65+']\n",
    "\n",
    "# Discrétisation de la variable Age\n",
    "data['Age_Binned'] = pd.cut(data['Age'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# 3Distribution (%) pour Age (discrétisé)\n",
    "dist_age = (\n",
    "    data['Age_Binned']\n",
    "    .value_counts(normalize=True)\n",
    "    .sort_index()\n",
    "    .reset_index()\n",
    "    .rename(columns={'index': 'Category', 'Age_Binned': 'Proportion'})\n",
    ")\n",
    "dist_age.columns = ['Category', 'Proportion']\n",
    "\n",
    "# Distribution (%) pour AgeOfPolicyHolder (déjà catégorielle)\n",
    "dist_policy = (\n",
    "    data['AgeOfPolicyHolder']\n",
    "    .value_counts(normalize=True)\n",
    "    .sort_index()\n",
    "    .reset_index()\n",
    "    .rename(columns={'index': 'Category', 'AgeOfPolicyHolder': 'Proportion'})\n",
    ")\n",
    "dist_policy.columns = ['Category', 'Proportion']\n",
    "\n",
    "# Création de sous-graphiques côte à côte\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=(\"Distribution of Age (Binned)\", \"Distribution of AgeOfPolicyHolder\"),\n",
    "    shared_yaxes=True\n",
    ")\n",
    "\n",
    "# Ajout du graphique pour Age\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=dist_age['Category'],\n",
    "        y=dist_age['Proportion'],\n",
    "        text=[f\"{p:.1%}\" for p in dist_age['Proportion']],\n",
    "        textposition='auto',\n",
    "        marker_color='skyblue',\n",
    "        name='Age'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Ajout du graphique pour AgeOfPolicyHolder\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=dist_policy['Category'],\n",
    "        y=dist_policy['Proportion'],\n",
    "        text=[f\"{p:.1%}\" for p in dist_policy['Proportion']],\n",
    "        textposition='auto',\n",
    "        marker_color='salmon',\n",
    "        name='AgeOfPolicyHolder'\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Mise en forme\n",
    "fig.update_layout(\n",
    "    title_text=\"Side-by-Side Comparison: Age vs AgeOfPolicyHolder\",\n",
    "    showlegend=False,\n",
    "    yaxis_tickformat='.0%',\n",
    "    height=500,\n",
    "    width=1000\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"Age Category\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Policy Holder Age Category\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Proportion\", row=1, col=1)\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f520bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# S'assurer qu'il n'y a pas de valeurs manquantes\n",
    "subset = data[['Age_Binned', 'AgeOfPolicyHolder']].dropna()\n",
    "\n",
    "# Créer la table de contingence\n",
    "contingency = pd.crosstab(subset['Age_Binned'], subset['AgeOfPolicyHolder'])\n",
    "\n",
    "# Appliquer le test du Khi-deux\n",
    "chi2, p, dof, expected = chi2_contingency(contingency)\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"Chi2 Statistic:\", round(chi2, 2))\n",
    "print(\"Degrees of Freedom:\", dof)\n",
    "print(\"p-value:\", p)\n",
    "\n",
    "# Optionnel : afficher la table de contingence\n",
    "print(\"\\nContingency Table:\")\n",
    "display(contingency)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8b5714",
   "metadata": {},
   "source": [
    "There is a strong dependency between the variables. We''ll use AgeofPolicyHolder to impute Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74eb98cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are  320 records where the age is 0\n",
    "print('There are ', len(data[data['Age']==0]), 'records where the age is 0')\n",
    "\n",
    "# For all this rows policy holder age  is between 16 and 17 years old.\n",
    "print('For all this rows policy holder age is: ', data.loc[(data['Age']==0),'AgeOfPolicyHolder'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc5831a",
   "metadata": {},
   "source": [
    "We  Will replace all 0 values with 16 or 17 randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45995d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_ages = np.random.choice([16, 17], size=data['Age'].eq(0).sum())\n",
    "data.loc[data['Age'] == 0, 'Age'] = random_ages\n",
    "\n",
    "print('Now there are ', len(data[data['Age']==0]), 'records where the age is 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688d2800",
   "metadata": {},
   "source": [
    "### Variable delay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72efe3c",
   "metadata": {},
   "source": [
    "We create a quantitative variable that will compute the distance between the day of the accident and the day of claim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dc7b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "months_map = {\n",
    "    'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6,\n",
    "    'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12\n",
    "}\n",
    "\n",
    "# Mapping des mois en nombres\n",
    "data['Month_num'] = data['Month'].map(months_map)\n",
    "data['MonthClaimed_num'] = data['MonthClaimed'].map(months_map)\n",
    "\n",
    "# Ajustement si MonthClaimed < Month (cas de passage à l’année suivante)\n",
    "data.loc[data['MonthClaimed_num'] <= data['Month_num'], 'MonthClaimed_num'] += 12\n",
    "\n",
    "# Calcul du délai approximatif en semaines\n",
    "data['delay_weeks'] = (\n",
    "    (data['MonthClaimed_num'] - data['Month_num']) * 5 +\n",
    "    (data['WeekOfMonthClaimed'] - data['WeekOfMonth'])\n",
    ")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241aa140",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e1deba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables quantitatives à analyser\n",
    "vars_quanti = [\"delay_weeks\", \"Age\"]\n",
    "\n",
    "for col in vars_quanti:\n",
    "    # --- Boxplot ---\n",
    "    fig_box = px.box(data,\n",
    "        x=\"FraudFound_P\",          \n",
    "        y=col,                   \n",
    "        color=\"FraudFound_P\",      \n",
    "        title=f\"Boxplot de {col} selon la fraude\"\n",
    "    )\n",
    "    fig_box.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b567c9ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c3d17c6",
   "metadata": {},
   "source": [
    "Correlations are low"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ae05a8",
   "metadata": {},
   "source": [
    "### Suppression des variables inutiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1902b9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [\n",
    "    'Month', 'WeekOfMonth', 'DayOfWeek', 'DayOfWeekClaimed', \n",
    "    'MonthClaimed', 'WeekOfMonthClaimed', 'PolicyNumber', \n",
    "    'RepNumber', 'Days_Policy_Claim', 'AgeOfPolicyHolder', \n",
    "    'BasePolicy', 'Year', \"VehicleCategory\", 'Month_num', 'MonthClaimed_num'\n",
    "]\n",
    "\n",
    "data = data.drop(columns=cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc170cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46731080",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5deb0eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "folder_path = \"/home/onyxia/work/PROJET_STATAPP/Data/Cleans\"\n",
    "file_path = os.path.join(folder_path, \"data_vehicle_cleaned.csv\")\n",
    "df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a893ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"/home/onyxia/work/PROJET_STATAPP/Data/Cleans/data_vehicle_cleaned.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedfb02d",
   "metadata": {},
   "source": [
    "# Pipeline de feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017bbe74",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6fd22a",
   "metadata": {},
   "source": [
    "Jusqu'ici, nous avons créé la variable delay_weeks, supprimé les variables qui ont nécessité sa création et d'autres variables non pertinentes. Nous avons par ailleurs, imputé les données manquantes sur Age. \n",
    "\n",
    "Nous procédons ici présent à l'encodage dans variables qualitatives, le scaling de Age et delay_weeks. \n",
    "\n",
    "Puis nous verrons le feature importance de chaque variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eb74a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modalités des variables\n",
    "exclude_cols = ['Age', 'delay_weeks', 'FraudFound_P']\n",
    "for col in df.columns:\n",
    "    if col not in exclude_cols:\n",
    "        print(f\"\\n--- {col} ---\")\n",
    "        print(data[col].unique())  # affiche toutes les modalités\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc45aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train-test-split\n",
    "X = df.drop(columns=['FraudFound_P'])\n",
    "y = df['FraudFound_P']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade20d4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5165f08a",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dca31dc",
   "metadata": {},
   "source": [
    "# Encodage des variables Make , PolicyType et MaritalStatus : df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77e92bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn import preprocessing\n",
    "# define Feature Hashing Vectorizer\n",
    "vectorizer_Make = HashingVectorizer(n_features=8, norm=None, alternate_sign=False, ngram_range=(1,1), binary=True)\n",
    "vectorizer_PolicyType = HashingVectorizer(n_features=4, norm=None, alternate_sign=False, ngram_range=(1,1), binary=True)\n",
    "\n",
    "vectorizer_MaritalStatus = HashingVectorizer(n_features=4, norm=None, alternate_sign=False, ngram_range=(1,1), binary=True)\n",
    "# fit the hashing vectorizer and transform the education column\n",
    "X_Make = vectorizer_Make.fit_transform(df[\"Make\"])\n",
    "X_PolicyType = vectorizer_PolicyType.fit_transform(df['PolicyType'])\n",
    "# transformed and raw column to data frame\n",
    "df_Make = pd.DataFrame(X_Make.toarray()).assign(Make = df[\"Make\"])\n",
    "df_Make.columns=['make_0','make_1','make_2','make_3','make_4', 'make_5', 'make_6', 'make_7', 'Make']\n",
    "df_PolicyType = pd.DataFrame(X_PolicyType.toarray()).assign(PolicyType = df['PolicyType'])\n",
    "df_PolicyType.columns=['PolicyType_0', 'PolicyType_1', 'PolicyType_2', 'PolicyType_3', 'PolicyType']\n",
    "df_make_PolicyType = pd.concat([df_Make, df_PolicyType], axis=1)\n",
    "\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit(df[\"MaritalStatus\"])\n",
    "\n",
    "MaritalStatus_one_hot_sklearn_binar = pd.DataFrame(lb.transform(X_train[\"MaritalStatus\"]), columns=lb.classes_)\n",
    "MaritalStatus_one_hot_sklearn_binar\n",
    "\n",
    "df.drop(['Make','MaritalStatus', 'PolicyType'], axis=1, inplace=True)\n",
    "df_make_PolicyType.drop(['Make', 'PolicyType'], axis=1, inplace=True)\n",
    "\n",
    "df = pd.concat([df, df_make_PolicyType, MaritalStatus_one_hot_sklearn_binar], axis=1, join='inner')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dedec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_ = list(df_make_PolicyType.columns) + list(MaritalStatus_one_hot_sklearn_binar.columns)\n",
    "columns_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac0a543",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffce668",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train-test-split\n",
    "X = df.drop(columns=['FraudFound_P'])\n",
    "y = df['FraudFound_P']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa61160",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# -----------------------------\n",
    "# Colonnes par type\n",
    "# -----------------------------\n",
    "ordinal_cols = ['VehiclePrice', 'Days_Policy_Accident', 'PastNumberOfClaims', \n",
    "                'AgeOfVehicle', 'NumberOfSuppliments', 'AddressChange_Claim', \n",
    "                'NumberOfCars', 'DriverRating', 'Deductible']\n",
    "\n",
    "binary_cols = ['AccidentArea','Sex','Fault','PoliceReportFiled','WitnessPresent','AgentType']\n",
    "scale_cols = ['Age', 'delay_weeks'] \n",
    "\n",
    "# Ajoutez ces variables si nécessaires:\n",
    "label_cols = ['MaritalStatus']  # À définir selon vos données\n",
    "freq_cols = []  # À définir selon vos données\n",
    "hash_cols = ['Make', 'PolicyType']  # Exemple de colonnes pour HashingVectorizer\n",
    "\n",
    "# -----------------------------\n",
    "# Transformers personnalisés\n",
    "# -----------------------------\n",
    "class HashingVectorizerWrapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_features):\n",
    "        self.n_features = n_features\n",
    "        self.vectorizer = HashingVectorizer(\n",
    "            n_features=n_features,\n",
    "            norm=None,\n",
    "            alternate_sign=False,\n",
    "            ngram_range=(1,1),\n",
    "            binary=True\n",
    "        )\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # X est un DataFrame -> convertir en série 1D de strings\n",
    "        X_str = X.iloc[:,0].astype(str)\n",
    "        transformed = self.vectorizer.transform(X_str)\n",
    "        # Retourner un array dense pour compatibilité\n",
    "        return transformed.toarray()\n",
    "\n",
    "class LabelBinarizerWrapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.lb = LabelBinarizer()\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.lb.fit(X.iloc[:,0])\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return self.lb.transform(X.iloc[:,0])\n",
    "\n",
    "class BinaryEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.binary_map = {\n",
    "            'No':0, 'Yes':1, 'Female':0, 'Male':1, 'Urban':1, 'Rural':0, \n",
    "            'Policy Holder':1, 'Third Party':0, 'External':0, 'Internal':1\n",
    "        }\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_trans = X.copy()\n",
    "        for col in X_trans.columns:\n",
    "            X_trans[col] = X_trans[col].map(self.binary_map)\n",
    "        return X_trans.values  # Retourner un array pour compatibilité\n",
    "\n",
    "class OrdinalMapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, mapping_dict):\n",
    "        self.mapping_dict = mapping_dict\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_trans = X.copy()\n",
    "        for col, map_dict in self.mapping_dict.items():\n",
    "            X_trans[col] = X_trans[col].map(map_dict)\n",
    "        return X_trans.values  # Retourner un array pour compatibilité\n",
    "\n",
    "# Mapping des colonnes ordinales\n",
    "ordinal_mapping = {\n",
    "    'VehiclePrice': {'less than 20000':0, '20000 to 29000':1, '30000 to 39000':2,\n",
    "                     '40000 to 59000':3, '60000 to 69000':4, 'more than 69000':5},\n",
    "    'Days_Policy_Accident': {'none':0, '1 to 7':1, '8 to 15':2, '15 to 30':3, 'more than 30':4},\n",
    "    'PastNumberOfClaims': {'none':0, '1':1, '2 to 4':2, 'more than 4':3},\n",
    "    'AgeOfVehicle': {'new':0, '2 years':1, '3 years':2, '4 years':3, \n",
    "                     '5 years':4, '6 years':5, '7 years':6, 'more than 7':7},\n",
    "    'NumberOfSuppliments': {'none':0, '1 to 2':1, '3 to 5':2, 'more than 5':3},\n",
    "    'AddressChange_Claim': {'no change':0,'under 6 months':1, '1 year':2, \n",
    "                           '2 to 3 years':3, '4 to 8 years':4},\n",
    "    'NumberOfCars': {'1 vehicle':0, '2 vehicles':1, '3 to 4':2, '5 to 8':3, 'more than 8':4},\n",
    "    'DriverRating': {1:1, 2:2, 3:3, 4:4},\n",
    "    'Deductible': {300:300, 400:400, 500:500, 700:700}\n",
    "}\n",
    "\n",
    "# Pipeline ordinal\n",
    "ordinal_pipe = Pipeline([\n",
    "    ('ordinal_map', OrdinalMapper(ordinal_mapping))\n",
    "])\n",
    "\n",
    "# -----------------------------\n",
    "# Pipeline complet CORRIGÉ\n",
    "# -----------------------------\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('binary', BinaryEncoder(), binary_cols),\n",
    "    ('ordinal', ordinal_pipe, ordinal_cols),\n",
    "    ('scale', StandardScaler(), scale_cols),\n",
    "    # Ajoutez d'autres transformers selon vos besoins:\n",
    "    # ('hashing', HashingVectorizerWrapper(n_features=8), ['Make']),\n",
    "    # ('label_bin', LabelBinarizerWrapper(), ['MaritalStatus'])\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "X_train_processed = pd.DataFrame(preprocessor.fit_transform(X_train))\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0d5712",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0051f4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b8caad",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Convertir en DataFrame avec noms de colonnes\n",
    "all_cols = binary_cols + ordinal_cols + scale_cols \n",
    "\n",
    "df1 = pd.DataFrame(X_train_processed).reset_index(drop=True)\n",
    "df1.columns = all_cols\n",
    "df2 = X_train[columns_].reset_index(drop=True)\n",
    "X_train_processed = pd.concat([df1, df2], axis=1, join='inner')\n",
    "\n",
    "\n",
    "\n",
    "df1_test = pd.DataFrame(X_test_processed).reset_index(drop=True)\n",
    "df1_test.columns = all_cols\n",
    "df2_test = X_test[columns_].reset_index(drop=True)\n",
    "X_test_processed = pd.concat([df1_test, df2_test], axis=1, join='inner')\n",
    "#X_test_processed = pd.concat([pd.DataFrame(X_test_processed, columns=all_cols), X_test[columns_]], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba44bfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49e20d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train_processed), len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d0dcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c75b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['delay_weeks'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247279af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2496e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe45a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_config(display='diagram')\n",
    "preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16684c3",
   "metadata": {},
   "source": [
    "# Modélisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb1681f",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9364c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0331231c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_processed.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f2a482",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = X_train_processed.copy()\n",
    "y1 = y_train.copy()\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "rf.fit(X1, y1)\n",
    "\n",
    "importances = pd.Series(rf.feature_importances_, index=X1.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,5))\n",
    "importances.head(15).plot(kind='bar', color='teal')\n",
    "plt.title(\"Top 15 Feature Importances - RandomForest\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f20d7f",
   "metadata": {},
   "source": [
    "### SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a7b438",
   "metadata": {},
   "source": [
    "The Synthetic Minority Over-sampling Technique (SMOTE) is a method used in machine learning to address the issue of imbalanced datasets. Imbalanced datasets are common in classification problems, especially Fraud Detection datasets, where one class (often the minority class) has significantly fewer instances than the other class(es). SMOTE aims to balance the class distribution by generating synthetic examples from the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e0067b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install --upgrade imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16554d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "print(\"Avant SMOTE :\", Counter(y_train))\n",
    "\n",
    "sm = SMOTE(sampling_strategy=0.25,  # 20% fraude,\n",
    "     random_state=42, \n",
    "     k_neighbors=5)\n",
    "X_train_res, y_train_res = sm.fit_resample(X_train_processed, y_train)\n",
    "\n",
    "print(\"Après SMOTE :\", Counter(y_train_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc70c614",
   "metadata": {},
   "source": [
    "## Modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a216e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Définir les modèles\n",
    "# -----------------------------\n",
    "models = {\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=200, random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),\n",
    "    \"LightGBM\": LGBMClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# Entraîner, évaluer et sauvegarder les modèles\n",
    "# -----------------------------\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n--- Training {name} ---\")\n",
    "    \n",
    "    # Entraînement\n",
    "    model.fit(X_train_res, y_train_res)\n",
    "    \n",
    "    # Sauvegarde\n",
    "    joblib.dump(model, f\"{name}.pkl\")\n",
    "    \n",
    "    # Prédiction\n",
    "    y_pred = model.predict(X_test_processed)\n",
    "    y_proba = model.predict_proba(X_test_processed)[:,1]  # probabilité pour AUC\n",
    "    \n",
    "    # Metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    \n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": acc,\n",
    "        \"Precision\": prec,\n",
    "        \"Recall\": rec,\n",
    "        \"F1-score\": f1,\n",
    "        \"AUC\": auc\n",
    "    })\n",
    "\n",
    "# -----------------------------\n",
    "# Tableau récapitulatif\n",
    "# -----------------------------\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values(by=\"F1-score\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n=== Résultats des modèles ===\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8fbe83",
   "metadata": {},
   "source": [
    "### Indice de Youden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbab74ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "lr_model = models[\"LogisticRegression\"]\n",
    "\n",
    "# Probabilité pour la classe positive\n",
    "y_proba = lr_model.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "# Calculer TPR, FPR pour tous les seuils\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "\n",
    "# Indice de Youden\n",
    "youden_index = tpr - fpr\n",
    "best_idx = np.argmax(youden_index)\n",
    "best_threshold = thresholds[best_idx]\n",
    "\n",
    "print(\"Meilleur seuil (indice de Youden) :\", best_threshold)\n",
    "print(\"TPR (Recall) à ce seuil :\", tpr[best_idx])\n",
    "print(\"FPR à ce seuil :\", fpr[best_idx])\n",
    "\n",
    "# Appliquer le seuil optimal\n",
    "y_pred_best = (y_proba >= best_threshold).astype(int)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred_best)\n",
    "prec = precision_score(y_test, y_pred_best)\n",
    "rec = recall_score(y_test, y_pred_best)\n",
    "f1 = f1_score(y_test, y_pred_best)\n",
    "auc = roc_auc_score(y_test, y_proba)  # AUC reste sur la probabilité\n",
    "\n",
    "print(\"\\n=== Métriques avec seuil optimal (Youden) ===\")\n",
    "print(f\"Accuracy: {acc:.3f}, Precision: {prec:.3f}, Recall: {rec:.3f}, F1-score: {f1:.3f}, AUC: {auc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a106a03f",
   "metadata": {},
   "source": [
    "## CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af18c27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c37ab9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Fonction pour obtenir le seuil optimal (Youden)\n",
    "def optimal_threshold_youden(y_true, y_proba):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_proba)\n",
    "    youden = tpr - fpr\n",
    "    idx = np.argmax(youden)\n",
    "    return thresholds[idx], tpr[idx], 1 - fpr[idx]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Définir les modèles\n",
    "# -----------------------------\n",
    "models = {\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=200, random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),\n",
    "    \"CatBoost\": CatBoostClassifier(\n",
    "        iterations=500,\n",
    "        depth=8,\n",
    "        learning_rate=0.05,\n",
    "        loss_function='Logloss',\n",
    "        eval_metric='AUC',\n",
    "        random_seed=42,\n",
    "        verbose=False,\n",
    "        class_weights=[1, 4]   # car 20% de fraude\n",
    "    )\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "# -----------------------------\n",
    "# Entraîner, évaluer, Youden\n",
    "# -----------------------------\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n--- Training {name} ---\")\n",
    "    \n",
    "    # 1. Entraînement\n",
    "    model.fit(X_train_res, y_train_res)\n",
    "    \n",
    "    # 2. Probabilités\n",
    "    y_proba = model.predict_proba(X_test_processed)[:, 1]\n",
    "    \n",
    "    # 3. Seuil optimal (Youden)\n",
    "    best_threshold, sens, spec = optimal_threshold_youden(y_test, y_proba)\n",
    "    \n",
    "    # 4. Nouvelle prédiction selon ce seuil\n",
    "    y_pred_opt = (y_proba >= best_threshold).astype(int)\n",
    "    \n",
    "    # 5. Calcul des métriques au seuil optimal\n",
    "    acc = accuracy_score(y_test, y_pred_opt)\n",
    "    prec = precision_score(y_test, y_pred_opt)\n",
    "    rec = recall_score(y_test, y_pred_opt)\n",
    "    f1 = f1_score(y_test, y_pred_opt)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    \n",
    "\n",
    "    # 6. Stocker les résultats\n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"Best Threshold (Youden)\": best_threshold,\n",
    "        \"Sensitivity\": sens,\n",
    "        \"Specificity\": spec,\n",
    "        \"Accuracy\": acc,\n",
    "        \"Precision\": prec,\n",
    "        \"Recall\": rec,\n",
    "        \"F1-score\": f1,\n",
    "        \"AUC\": auc\n",
    "    })\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Tableau récapitulatif\n",
    "# -----------------------------\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values(by=\"F1-score\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n=== Résultats des modèles (au seuil optimal) ===\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a508321",
   "metadata": {},
   "source": [
    "## Optimisation des hyperparamètres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a29ec6e",
   "metadata": {},
   "source": [
    "### Catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533e2a3f",
   "metadata": {},
   "source": [
    "| Hyperparamètres       | Description                                | \n",
    "| --------------------- | ------------------------------------------ | \n",
    "| `iterations`          | Nombre d’arbres                            | \n",
    "| `depth`               | Profondeur maximale des arbres             | \n",
    "| `learning_rate`       | Taux d’apprentissage                       | \n",
    "| `l2_leaf_reg`         | Régularisation L2                | \n",
    "| `border_count`        | Nombre de bins pour les features continues | \n",
    "| `class_weights`       | Importance des classes                     | \n",
    "| `bagging_temperature` | Pour booster la variance et régularisation | \n",
    "| `random_strength`     | Bruit aléatoire dans la construction       | 0–1                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b2cb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Définition du modèle de base\n",
    "cat = CatBoostClassifier(\n",
    "    loss_function='Logloss',\n",
    "    eval_metric='AUC',\n",
    "    random_seed=42,\n",
    "    verbose=0,\n",
    "    class_weights=[1, 4]  # ratio 20% fraude\n",
    ")\n",
    "\n",
    "# Grille d'hyperparamètres\n",
    "param_grid = {\n",
    "    'iterations': [100,300,500],\n",
    "    'depth': [6, 8, 10],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'l2_leaf_reg': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=cat,\n",
    "    param_grid=param_grid,\n",
    "    scoring='roc_auc',\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Entraînement\n",
    "grid_search.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Meilleurs paramètres\n",
    "print(\"Meilleurs paramètres CatBoost :\", grid_search.best_params_)\n",
    "print(\"Meilleure AUC CV :\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b081d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupération du meilleur modèle\n",
    "best_cat = grid_search.best_estimator_\n",
    "\n",
    "# Probabilités pour le test\n",
    "y_proba = best_cat.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "# Calcul du seuil optimal (Youden)\n",
    "from sklearn.metrics import roc_curve\n",
    "def optimal_threshold_youden(y_true, y_proba):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_proba)\n",
    "    youden = tpr - fpr\n",
    "    idx = np.argmax(youden)\n",
    "    return thresholds[idx], tpr[idx], 1 - fpr[idx]\n",
    "\n",
    "best_threshold, sens, spec = optimal_threshold_youden(y_test, y_proba)\n",
    "y_pred_opt = (y_proba >= best_threshold).astype(int)\n",
    "\n",
    "# Métriques finales\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "acc = accuracy_score(y_test, y_pred_opt)\n",
    "prec = precision_score(y_test, y_pred_opt)\n",
    "rec = recall_score(y_test, y_pred_opt)\n",
    "f1 = f1_score(y_test, y_pred_opt)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "print(\"\\n=== Résultats CatBoost optimisé ===\")\n",
    "print(f\"Best Threshold (Youden): {best_threshold:.3f}\")\n",
    "print(f\"Sensitivity: {sens:.3f}, Specificity: {spec:.3f}\")\n",
    "print(f\"Accuracy: {acc:.3f}, Precision: {prec:.3f}, Recall: {rec:.3f}, F1: {f1:.3f}, AUC: {auc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd4832a",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24f7dad",
   "metadata": {},
   "source": [
    "| Hyperparamètre      | Description                                                                                          |\n",
    "| ------------------- | ---------------------------------------------------------------------------------------------------- |\n",
    "| `num_leaves`        | Nombre maximum de feuilles par arbre (contrôle la complexité, équivalent à la profondeur effective). |\n",
    "| `max_depth`         | Profondeur maximale des arbres (permet de limiter la complexité, -1 = illimité).                     |\n",
    "| `learning_rate`     | Taux d’apprentissage, plus petit = modèle plus robuste mais nécessite plus d’arbres.                 |\n",
    "| `n_estimators`      | Nombre total d’arbres (boosting rounds).                                                             |\n",
    "| `min_data_in_leaf`  | Nombre minimum d’observations dans une feuille (augmente la régularisation).                         |\n",
    "| `feature_fraction`  | Fractions de variables à utiliser pour chaque arbre (équivalent au “colsample”).                     |\n",
    "| `bagging_fraction`  | Fraction d’observations utilisées pour chaque arbre (sous-échantillonnage).                          |\n",
    "| `bagging_freq`      | Fréquence du bagging (0 = désactivé, >0 = ex : 5 = appliquer toutes les 5 itérations).               |\n",
    "| `lambda_l1`         | Régularisation L1 (LASSO) sur les feuilles.                                                          |\n",
    "| `lambda_l2`         | Régularisation L2 (Ridge) sur les feuilles.                                                          |\n",
    "| `min_gain_to_split` | Gain minimal pour créer une nouvelle division (pruning).                                             |\n",
    "| `class_weight`      | Gestion du déséquilibre : ex. `{0:1, 1:5}` ou `\"balanced\"`.                                          |\n",
    "| `subsample_for_bin` | Taille de l’échantillon utilisé pour créer les histogrammes de binning.                              |\n",
    "| `max_bin`           | Nombre maximal de bins pour les features continues (granularité des splits).                         |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998afc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# -----------------------------\n",
    "# Définition du modèle de base\n",
    "# -----------------------------\n",
    "lgbm = LGBMClassifier(\n",
    "    objective='binary',\n",
    "    random_state=42,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Grille d'hyperparamètres\n",
    "# -----------------------------\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 300, 500],\n",
    "    'max_depth': [6, 8, 10],               \n",
    "    'learning_rate': [0.01, 0.05, 0.1],   \n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# GridSearchCV\n",
    "# -----------------------------\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=lgbm,\n",
    "    param_grid=param_grid,\n",
    "    scoring='roc_auc',\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Entraînement\n",
    "# -----------------------------\n",
    "grid_search.fit(X_train_res, y_train_res)\n",
    "\n",
    "# -----------------------------\n",
    "# Meilleurs paramètres\n",
    "# -----------------------------\n",
    "print(\"Meilleurs paramètres LightGBM :\", grid_search.best_params_)\n",
    "print(\"Meilleure AUC CV :\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f09257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupération du meilleur modèle LightGBM\n",
    "best_lgbm = grid_search.best_estimator_\n",
    "\n",
    "# Probabilités pour le test\n",
    "y_proba = best_lgbm.predict_proba(X_test_processed)[:, 1]\n",
    "\n",
    "# ---- Calcul du seuil optimal (Youden) ----\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "def optimal_threshold_youden(y_true, y_proba):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_proba)\n",
    "    youden = tpr - fpr\n",
    "    idx = np.argmax(youden)\n",
    "    return thresholds[idx], tpr[idx], 1 - fpr[idx]\n",
    "\n",
    "best_threshold, sens, spec = optimal_threshold_youden(y_test, y_proba)\n",
    "\n",
    "# Prédictions avec le seuil optimal\n",
    "y_pred_opt = (y_proba >= best_threshold).astype(int)\n",
    "\n",
    "# ---- Métriques finales ----\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred_opt)\n",
    "prec = precision_score(y_test, y_pred_opt)\n",
    "rec = recall_score(y_test, y_pred_opt)\n",
    "f1 = f1_score(y_test, y_pred_opt)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "print(\"\\n=== Résultats LightGBM optimisé ===\")\n",
    "print(f\"Best Threshold (Youden): {best_threshold:.3f}\")\n",
    "print(f\"Sensitivity: {sens:.3f}, Specificity: {spec:.3f}\")\n",
    "print(f\"Accuracy: {acc:.3f}, Precision: {prec:.3f}, Recall: {rec:.3f}, F1: {f1:.3f}, AUC: {auc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df49c008",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f4aab5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba949686",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
